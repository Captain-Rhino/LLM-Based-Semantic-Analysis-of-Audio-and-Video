import os
import subprocess
import whisperx
import json

# Step 1: æå–éŸ³é¢‘
video_path = "your_video.mp4"
audio_path = "audio.wav"
subprocess.call(['ffmpeg', '-y', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', audio_path])

# Step 2: WhisperX åŠ è½½æ¨¡å‹
device = "cuda" if whisperx.utils.get_device() == "cuda" else "cpu"
model = whisperx.load_model("large-v2", device)

# Step 3: æ‰§è¡Œè¯­éŸ³è¯†åˆ«
result = model.transcribe(audio_path)
print("ğŸ”¤ åˆæ­¥è¯†åˆ«å®Œæˆã€‚")

# Step 4: åŠ è½½è¯´è¯äººè¯†åˆ«æ¨¡å‹ï¼ˆDiarizationï¼‰
diarize_model = whisperx.DiarizationPipeline(use_auth_token="your_hf_token", device=device)
diarize_segments = diarize_model(audio_path)

# Step 5: å¯¹é½è¯´è¯äººæ ‡è®°
result_aligned = whisperx.align(
    result["segments"], model.model, model.tokenizer, audio_path, device
)
result_with_speaker = whisperx.assign_speakers(result_aligned["segments"], diarize_segments)

# Step 6: ä¿å­˜ç»“æ„åŒ– JSON
output_json = "multi_speaker_transcript.json"
with open(output_json, "w", encoding="utf-8") as f:
    json.dump(result_with_speaker, f, ensure_ascii=False, indent=2)

print(f"\nâœ… å¤šè¯´è¯äººè¯­éŸ³è½¬æ–‡å­—å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°ï¼š{output_json}")
