from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import jieba
import numpy as np
from PIL import Image
import os
import json

def load_stopwords(file_path):
    """
    ä»txtæ–‡ä»¶åŠ è½½åœç”¨è¯åˆ—è¡¨
    """
    with open(file_path, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())


def generate_wordcloud(transcription, summary_path, output_dir, video_name):
    """
    ç»“åˆè¯­éŸ³è½¬å½•å’Œè§†é¢‘æ€»ç»“ç”Ÿæˆè¯äº‘
    :param transcription: è¯­éŸ³è¯†åˆ«ç»“æœåˆ—è¡¨
    :param summary_path: è§†é¢‘æ€»ç»“JSONæ–‡ä»¶è·¯å¾„
    :param output_dir: è¾“å‡ºç›®å½•
    :param video_name: è§†é¢‘åç§°
    """
    # 1. åŠ è½½æ€»ç»“æ–‡æœ¬
    with open(summary_path, 'r', encoding='utf-8') as f:
        summary_data = json.load(f)
        summary_text = summary_data.get("summary", "")

    # 2. åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼ˆè½¬å½•+æ€»ç»“ï¼‰
    transcript_text = " ".join([seg["text"] for seg in transcription if seg.get("text")])
    combined_text = f"{transcript_text} {summary_text}"

    #3. åœç”¨è¯è·¯å¾„
    stopwords_path = r"G:\videochat\my_design\baidu_stopwords.txt"
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
    stopwords = load_stopwords(stopwords_path) # è¡¥å……å¸¸ç”¨åœç”¨è¯

    # 4. é«˜çº§æ–‡æœ¬å¤„ç†
    words = []
    for word in jieba.lcut(combined_text):
        if len(word) > 1 and word not in stopwords and not word.isspace():
            # å¤„ç†ç‰¹æ®Šç¬¦å·å’Œemoji
            clean_word = ''.join(c for c in word if c.isalnum() or c in ['#', '@'])
            if clean_word:
                words.append(clean_word)

    # 4. è¯é¢‘ç»Ÿè®¡ï¼ˆå¸¦æƒé‡ï¼‰
    word_freq = Counter(words)

    # 5. ç”Ÿæˆè¯äº‘ï¼ˆå¸¦æ ·å¼ä¼˜åŒ–ï¼‰
    wc = WordCloud(
        font_path="msyh.ttc",  # å¾®è½¯é›…é»‘
        width=1200,
        height=800,
        background_color="white",
        colormap="viridis",  # ä½¿ç”¨ç§‘å­¦é…è‰²
        max_words=200,
        collocations=False,  # é¿å…è¯ç»„é‡å¤
        prefer_horizontal=0.8,  # æ¨ªå‘æ–‡å­—æ¯”ä¾‹
        mask=np.array(Image.open("cloud_mask.png")) if os.path.exists("cloud_mask.png") else None
    ).generate_from_frequencies(word_freq)

    # 6. ä¿å­˜ç»“æœ
    output_path = os.path.join(output_dir, f"{video_name}_combined_wordcloud.png")
    wc.to_file(output_path)

    # åŒæ—¶ä¿å­˜å¤„ç†åçš„æ–‡æœ¬å’Œè¯é¢‘
    with open(os.path.join(output_dir, f"{video_name}_processed_text.txt"), "w", encoding="utf-8") as f:
        f.write(combined_text)

    with open(os.path.join(output_dir, f"{video_name}_word_freq.json"), "w", encoding="utf-8") as f:
        json.dump(word_freq, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç»¼åˆè¯äº‘å·²ç”Ÿæˆï¼š{output_path}")
    return output_path



from graphviz import Digraph


def generate_mindmap(word_freq, output_dir, video_name, top_k=10):
    """
    æ ¹æ®è¯é¢‘ç”Ÿæˆç®€å•æ€ç»´å¯¼å›¾ï¼ˆä¸»è¯+å­èŠ‚ç‚¹ï¼‰
    :param word_freq: è¯é¢‘ Counter å­—å…¸
    :param output_dir: è¾“å‡ºè·¯å¾„
    :param video_name: è§†é¢‘åç§°
    :param top_k: æ˜¾ç¤ºå‰å‡ ä¸ªä¸»å…³é”®è¯
    """
    dot = Digraph(comment='Mindmap', format='png')
    dot.attr('node', shape='box', fontname="Microsoft YaHei")

    center = f"{video_name}_ä¸»é¢˜"
    dot.node(center)

    # Top k é«˜é¢‘è¯ä½œä¸ºä¸€çº§ä¸»é¢˜
    for i, (word, freq) in enumerate(word_freq.most_common(top_k)):
        dot.node(word, f"{word} ({freq})")
        dot.edge(center, word)

    output_path = os.path.join(output_dir, f"{video_name}_mindmap")
    dot.render(output_path, cleanup=True)

    print(f"ğŸ§  æ€ç»´å¯¼å›¾å·²ç”Ÿæˆï¼š{output_path}.png")
    return output_path + ".png"
