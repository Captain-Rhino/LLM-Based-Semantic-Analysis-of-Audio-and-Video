import base64
import requests
import time
import json
from dashscope import MultiModalConversation

def build_structured_prompt(frame_info, is_last=False):
    base = (
        f"å½“å‰æ˜¯ç¬¬ {frame_info['segment_index']} æ®µï¼Œè¿™æ®µçš„è¯­éŸ³æ–‡æœ¬æ˜¯ï¼šâ€œ{frame_info['text']}â€ï¼Œ"
        f"è¯­éŸ³èµ·å§‹æ—¶é—´æ˜¯ {frame_info['start']} ç§’ï¼Œè¯­éŸ³ç»“æŸæ—¶é—´æ˜¯ {frame_info['end']} ç§’ï¼Œ"
        f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame_info['timestamp']} ç§’å–å¾—ï¼Œ"
    )
    if is_last:
        return base + "è¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œå½“å‰æ˜¯æœ€åä¸€ä¸ªå…³é”®å¸§ï¼Œè¯·ç»“åˆå‰é¢çš„å…³é”®å¸§å’Œä¿¡æ¯æ¥æ€»ç»“è¯¥è§†é¢‘å†…å®¹ã€‚"
    else:
        return base + "è¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œå…ˆä¸æè¿°ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"

def generate_video_summary(image_path, text, api_key):
    messages = [
        {
            "role": "user",
            "content": [
                {"image": image_path},
                {"text": text}
            ],
        }
    ]

    # å‘èµ·è¯·æ±‚
    response = MultiModalConversation.call(
        api_key=api_key,
        model='qwen-vl-plus-latest',
        messages=messages
    )

    print("ğŸ¬ è§†é¢‘å†…å®¹æ€»ç»“ï¼š", json.dumps(response, ensure_ascii=False, indent=2))

    if response and response.get('output') and response['output'].get('choices'):
        return response['output']['choices'][0]['message']['content'][0]['text']
    else:
        print("âŒ é”™è¯¯ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœ")
        print("è¿”å›å†…å®¹ï¼š", response)
        return "æ— æ³•ç”Ÿæˆæ€»ç»“"

def summarize_video_from_all_frames(keyframes_combined, api_key, output_summary_path=None):
    messages = []
    for i, frame in enumerate(keyframes_combined):
        prompt = (
            f"å½“å‰æ˜¯ç¬¬ {frame['segment_index']} æ®µï¼Œè¿™æ®µçš„è¯­éŸ³æ–‡æœ¬æ˜¯ï¼šâ€œ{frame['text']}â€ï¼Œ"
            f"è¯­éŸ³èµ·å§‹æ—¶é—´æ˜¯ {frame['start']} ç§’ï¼Œè¯­éŸ³ç»“æŸæ—¶é—´æ˜¯ {frame['end']} ç§’ï¼Œ"
            f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œè¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œå…ˆä¸æè¿°ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
        )

        print(prompt)

        response = MultiModalConversation.call(
            api_key=api_key,
            model='qwen-vl-plus-latest',
            messages=[{
                "role": "user",
                "content": [
                    {"image": frame['image_path']},
                    {"text": prompt}
                ]
            }]
        )

        if response and response.get('output') and response['output'].get('choices'):
            reply = response['output']['choices'][0]['message']['content'][0]['text']
        else:
            reply = "âŒ é”™è¯¯ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœ"
        print("ğŸ¬ è§†é¢‘å†…å®¹æ€»ç»“ï¼š", reply)
        #time.sleep(1)

        messages.append({
            "role": "user",
            "content": [
                {"image": frame['image_path']},
                {"text": prompt}
            ]
        })
        messages.append({
            "role": "assistant",
            "content": [{"text": reply}]
        })

    messages.append({
        "role": "user",
        "content": [{"text": "è¯·ä½ æ ¹æ®ä»¥ä¸Šæ‰€æœ‰å›¾æ–‡å†…å®¹ï¼Œå¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œæ€»ç»“ã€‚"}]
    })

    response = MultiModalConversation.call(
        api_key=api_key,
        model='qwen-vl-plus-latest',
        messages=messages
    )

    #print("\nğŸ§  æœ€åä¸€è½®æ€»ç»“è°ƒç”¨å“åº”ï¼š", json.dumps(response, ensure_ascii=False, indent=2))

    try:
        summary = response['output']['choices'][0]['message']['content'][0]['text']
        print("\nğŸ“½ï¸ è§†é¢‘æ€»ç»“å®Œæˆï¼š\n", summary)

        if output_summary_path:
            with open(output_summary_path, "w", encoding="utf-8") as f:
                json.dump({"summary": summary}, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… æ€»ç»“å·²ä¿å­˜è‡³ï¼š{output_summary_path}")

        return summary

    except Exception as e:
        print("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥æˆ–æœªè¿”å›æœ‰æ•ˆç»“æœ")
        print("è¿”å›å†…å®¹ï¼š", response)
        return None
