import base64
import requests
import time
import json
from dashscope import MultiModalConversation

def build_structured_prompt(frame_info, is_last=False):
    if frame_info.get("mode") == "text_guided":
        # æ–‡æœ¬å¼•å¯¼æ¨¡å¼Prompt
        base = (
            f"å½“å‰æ˜¯ç¬¬ {frame_info['segment_index']} æ®µï¼Œè¯­éŸ³æ–‡æœ¬ï¼šâ€œ{frame_info['text']}â€ï¼Œ"
            f"æ—¶é—´åŒºé—´ {frame_info['start']}-{frame_info['end']}ç§’ï¼Œ"
            f"å›¾åƒæ‹æ‘„äº {frame_info['timestamp']}ç§’ï¼Œ"
        )
    else:
        # è§†è§‰å¼•å¯¼æ¨¡å¼Prompt
        base = (
            f"å½“å‰ç”»é¢æ‹æ‘„äºè§†é¢‘ç¬¬ {frame_info['timestamp']}ç§’ï¼Œ"
            f"è§†è§‰é‡è¦æ€§è¯„åˆ† {frame_info.get('importance', 0):.1f}ï¼Œ"
        )

    return base + ("è¯·æ€»ç»“è§†é¢‘å†…å®¹ã€‚" if is_last else "è¯·ä½ ç†è§£è¯¥è§†é¢‘ç‰‡æ®µçš„å¸§ä¿¡æ¯å’Œæ–‡æœ¬ï¼Œå…ˆä¸æè¿°ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚")
    # base = (
    #     f"å½“å‰æ˜¯ç¬¬ {frame_info['segment_index']} æ®µï¼Œè¿™æ®µçš„è¯­éŸ³æ–‡æœ¬æ˜¯ï¼šâ€œ{frame_info['text']}â€ï¼Œ"
    #     f"è¯­éŸ³èµ·å§‹æ—¶é—´æ˜¯ {frame_info['start']} ç§’ï¼Œè¯­éŸ³ç»“æŸæ—¶é—´æ˜¯ {frame_info['end']} ç§’ï¼Œ"
    #     f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame_info['timestamp']} ç§’å–å¾—ï¼Œ"
    # )
    # if is_last:
    #     return base + "è¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œå½“å‰æ˜¯æœ€åä¸€ä¸ªå…³é”®å¸§ï¼Œè¯·ç»“åˆå‰é¢çš„å…³é”®å¸§å’Œä¿¡æ¯æ¥æ€»ç»“è¯¥è§†é¢‘å†…å®¹ã€‚"
    # else:
    #     return base + "è¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œå…ˆä¸æè¿°ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"

def generate_video_summary(image_path, text, api_key):
    messages = [
        {
            "role": "user",
            "content": [
                {"image": image_path},
                {"text": text}
            ],
        }
    ]

    # å‘èµ·è¯·æ±‚
    response = MultiModalConversation.call(
        api_key=api_key,
        model='qwen-vl-plus-latest',
        messages=messages
    )

    print("ğŸ¬ è§†é¢‘å†…å®¹æ€»ç»“ï¼š", json.dumps(response, ensure_ascii=False, indent=2))

    if response and response.get('output') and response['output'].get('choices'):
        return response['output']['choices'][0]['message']['content'][0]['text']
    else:
        print("âŒ é”™è¯¯ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœ")
        print("è¿”å›å†…å®¹ï¼š", response)
        return "æ— æ³•ç”Ÿæˆæ€»ç»“"

def summarize_video_from_all_frames(keyframes_combined, api_key, output_summary_path=None):
    messages = []
    for i, frame in enumerate(keyframes_combined):
        prompt = (
            f"å½“å‰æ˜¯ç¬¬ {frame['segment_index']} æ®µï¼Œè¿™æ®µçš„è¯­éŸ³æ–‡æœ¬æ˜¯ï¼šâ€œ{frame['text']}â€ï¼Œ"
            f"è¯­éŸ³èµ·å§‹æ—¶é—´æ˜¯ {frame['start']} ç§’ï¼Œè¯­éŸ³ç»“æŸæ—¶é—´æ˜¯ {frame['end']} ç§’ï¼Œ"
            f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œè¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œå…ˆä¸æè¿°ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
        )

        print(prompt)

        response = MultiModalConversation.call(
            api_key=api_key,
            model='qwen-vl-plus-latest',
            messages=[{
                "role": "user",
                "content": [
                    {"image": frame['image_path']},
                    {"text": prompt}
                ]
            }]
        )

        if response and response.get('output') and response['output'].get('choices'):
            reply = response['output']['choices'][0]['message']['content'][0]['text']
        else:
            reply = "âŒ é”™è¯¯ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœ"
        print("ğŸ¬ è§†é¢‘å†…å®¹æ€»ç»“ï¼š", reply)
        #time.sleep(1)

        messages.append({
            "role": "user",
            "content": [
                {"image": frame['image_path']},
                {"text": prompt}
            ]
        })
        messages.append({
            "role": "assistant",
            "content": [{"text": reply}]
        })

    messages.append({
        "role": "user",
        "content": [{"text": "è¯·ä½ æ ¹æ®ä»¥ä¸Šæ‰€æœ‰å›¾æ–‡å†…å®¹ï¼Œå¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œæ€»ç»“ã€‚"}]
    })

    response = MultiModalConversation.call(
        api_key=api_key,
        model='qwen-vl-plus-latest',
        messages=messages
    )

    #print("\nğŸ§  æœ€åä¸€è½®æ€»ç»“è°ƒç”¨å“åº”ï¼š", json.dumps(response, ensure_ascii=False, indent=2))

    try:
        summary = response['output']['choices'][0]['message']['content'][0]['text']
        print("\nğŸ“½ï¸ è§†é¢‘æ€»ç»“å®Œæˆï¼š\n", summary)

        if output_summary_path:
            with open(output_summary_path, "w", encoding="utf-8") as f:
                json.dump({"summary": summary}, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… æ€»ç»“å·²ä¿å­˜è‡³ï¼š{output_summary_path}")

        return summary

    except Exception as e:
        print("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥æˆ–æœªè¿”å›æœ‰æ•ˆç»“æœ")
        print("è¿”å›å†…å®¹ï¼š", response)
        return None






def summarize_video_from_all_frames(keyframes_combined, api_key, output_summary_path=None):
    """
    ä½¿ç”¨å¤šè½®å›¾æ–‡å¯¹è¯æ„å»ºä¸Šä¸‹æ–‡ï¼Œæœ€åä¸€è½®æ€»ç»“æ•´æ®µè§†é¢‘å†…å®¹ã€‚
    :param keyframes_combined: åŒ…å«æ¯ä¸€å¸§å›¾æ–‡ä¿¡æ¯çš„åˆ—è¡¨
    :param api_key: DashScope çš„ API Key
    :param output_summary_path: å¯é€‰ï¼Œæœ€ç»ˆæ€»ç»“ä¿å­˜çš„è·¯å¾„
    """
    messages = []

    for i, frame in enumerate(keyframes_combined):
        if frame.get("mode") =="text_guided":
            prompt = (
                f"å½“å‰æ˜¯ç¬¬ {frame['segment_idx']} æ®µï¼Œè¿™æ®µçš„è¯­éŸ³æ–‡æœ¬æ˜¯ï¼šâ€œ{frame['text']}â€ï¼Œ"
                f"è¯­éŸ³èµ·å§‹æ—¶é—´æ˜¯ {frame['start']} ç§’ï¼Œè¯­éŸ³ç»“æŸæ—¶é—´æ˜¯ {frame['end']} ç§’ï¼Œ"
                f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œè¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œä¿æŒæ²‰é»˜ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
            )
        elif frame.get("mode") == "visual_compensate":
            prompt = (
                f"å½“å‰é™é»˜åŒºé—´çš„è§†è§‰è¡¥å¿å¸§"
                f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œè¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œä¿æŒæ²‰é»˜ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
            )
        else:
            prompt=(
                f"å½“å‰æ˜¯åŸºäºè§†è§‰æ˜¾è‘—æ€§æŠ½å–çš„å…³é”®å¸§ï¼Œå›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œ"
                f"è¯·ä½ è§‚å¯Ÿè¯¥å›¾åƒï¼Œç†è§£å…¶å†…å®¹ï¼Œä¿æŒæ²‰é»˜ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
            )
        print(prompt)

        user_msg = {
            "role": "user",
            "content": [
                {"image": frame['image_path']},
                {"text": prompt}
            ]
        }

        # è°ƒç”¨æ¥å£æ¨¡æ‹Ÿâ€œè®°ä½è¿™ä¸€å¸§â€
        response = MultiModalConversation.call(
            api_key=api_key,
            model='qwen-vl-plus-latest',
            messages=[user_msg]
        )

        if response and response.get('output') and response['output'].get('choices'):
            reply = response['output']['choices'][0]['message']['content'][0]['text']
        else:
            reply = "âŒ é”™è¯¯ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœ"
            print("è¿”å›å†…å®¹ï¼š", response)

        print("ğŸ¬ è§†é¢‘å†…å®¹æ€»ç»“ï¼š", reply)

        messages.append(user_msg)
        messages.append({
            "role": "assistant",
            "content": [{"text": reply}]
        })

        time.sleep(1)  # é˜²æ­¢è§¦å‘ QPS é™åˆ¶

    # æ·»åŠ æœ€ç»ˆæ€»ç»“è¯·æ±‚
    final_prompt = "è¯·ä½ æ ¹æ®ä»¥ä¸Šæ‰€æœ‰å›¾æ–‡å†…å®¹ï¼Œå¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œæ€»ç»“ã€‚"
    print("\nğŸ§  æœ€åä¸€è½®æ€»ç»“è¯·æ±‚ï¼š", final_prompt)

    messages.append({
        "role": "user",
        "content": [{"text": final_prompt}]
    })

    response = MultiModalConversation.call(
        api_key=api_key,
        model='qwen-vl-plus-latest',
        messages=messages
    )

    try:
        summary = response['output']['choices'][0]['message']['content'][0]['text']
        print("\nğŸ“½ï¸ è§†é¢‘æ€»ç»“å®Œæˆï¼š\n", summary)

        if output_summary_path:
            with open(output_summary_path, "w", encoding="utf-8") as f:
                json.dump({"summary": summary}, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… æ€»ç»“å·²ä¿å­˜è‡³ï¼š{output_summary_path}")

        return summary

    except Exception as e:
        print("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥æˆ–æœªè¿”å›æœ‰æ•ˆç»“æœ")
        print("è¿”å›å†…å®¹ï¼š", response)
        return None

