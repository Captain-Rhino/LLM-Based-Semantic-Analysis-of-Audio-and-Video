import base64
import requests
import time
import json
import os
import torch
from dashscope import MultiModalConversation


def summarize_video_from_all_frames(keyframes_combined, api_key, adaptor_path=None, output_summary_path=None):
    # è®¾ç½®GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"


    # åŠ è½½é€‚é…å±‚
    adaptor = None
    if adaptor_path and os.path.exists(adaptor_path):
        from A_clip_finetune import ClipAdaptor
        adaptor = ClipAdaptor()
        adaptor.load_state_dict(torch.load(adaptor_path, map_location=device))
        adaptor.to(device)
        adaptor.eval()
        print(f"ğŸ”§ é€‚é…å±‚å·²åŠ è½½ï¼š{adaptor_path}")
    else:
        print("âš ï¸ æ²¡æœ‰ä½¿ç”¨ClipAdaptorï¼Œç›´æ¥ç”¨åŸå§‹å›¾åƒç‰¹å¾")

    messages = []

    for frame in keyframes_combined:
        feat_data = torch.load(frame["feat_path"])
        image_feat = feat_data["image_feat"]
        if adaptor:
            image_feat = image_feat.to(device)
            if image_feat.dtype != torch.float32:
                image_feat = image_feat.float()
            image_feat = adaptor(image_feat).cpu()

        # æ„é€  prompt
        if frame.get("mode") == "text_guided":
            prompt = (
                f"å½“å‰æ˜¯ç¬¬ {frame['segment_idx']} æ®µï¼Œè¿™æ®µçš„è¯­éŸ³æ–‡æœ¬æ˜¯ï¼šâ€œ{frame['text']}â€ï¼Œ"
                f"è¯­éŸ³èµ·å§‹æ—¶é—´æ˜¯ {frame['start']} ç§’ï¼Œè¯­éŸ³ç»“æŸæ—¶é—´æ˜¯ {frame['end']} ç§’ï¼Œ"
                f"å›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œè¯·ä½ ç†è§£è¯¥å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œä¿æŒæ²‰é»˜ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
            )
        elif frame.get("mode") == "visual_compensate":
            prompt = (
                f"å½“å‰é™é»˜åŒºé—´çš„è§†è§‰è¡¥å¿å¸§ï¼Œå›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œè¯·ä½ ç†è§£è¯¥å›¾ç‰‡ï¼Œä¿æŒæ²‰é»˜ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
            )
        else:
            prompt = (
                f"å½“å‰æ˜¯åŸºäºè§†è§‰æ˜¾è‘—æ€§æŠ½å–çš„å…³é”®å¸§ï¼Œå›¾åƒåœ¨è¯¥è§†é¢‘çš„ç¬¬ {frame['timestamp']} ç§’å–å¾—ï¼Œ"
                f"è¯·ä½ è§‚å¯Ÿè¯¥å›¾åƒï¼Œç†è§£å…¶å†…å®¹ï¼Œä¿æŒæ²‰é»˜ï¼Œç­‰å¾…åç»­æŒ‡ä»¤ã€‚"
            )

        user_msg = {
            "role": "user",
            "content": [
                {"image": frame["image_path"]},
                {"text": prompt}
            ]
        }

        # æ‰“å° prompt
        print("\nğŸ“ Prompt:")
        print(prompt)

        # æ¨¡å‹è®°å¿†è¿™ä¸€å¸§
        response = MultiModalConversation.call(
            api_key=api_key,
            model='qwen-vl-plus-latest',
            messages=[user_msg]
        )

        if response and response.get('output') and response['output'].get('choices'):
            reply = response['output']['choices'][0]['message']['content'][0]['text']
        else:
            reply = "âŒ æ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœ"
            print("è¿”å›å†…å®¹ï¼š", response)

        # æ‰“å°æ¨¡å‹å›å¤
        print("âœ…æ¨¡å‹è¿”å›ç»“æœï¼š")
        print(reply)

        messages.append(user_msg)
        messages.append({
            "role": "assistant",
            "content": [{"text": reply}]
        })

        time.sleep(1)  # æ§åˆ¶èŠ‚å¥

    # æœ€åä¸€è½®æ€»ç»“
    final_prompt = "è¯·ä½ æ ¹æ®ä»¥ä¸Šæ‰€æœ‰å›¾æ–‡å†…å®¹ï¼Œå¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œæ€»ç»“ã€‚"
    messages.append({
        "role": "user",
        "content": [{"text": final_prompt}]
    })

    response = MultiModalConversation.call(
        api_key=api_key,
        model='qwen-vl-plus-latest',
        messages=messages
    )

    try:
        summary = response['output']['choices'][0]['message']['content'][0]['text']
        print("\nğŸ“½ï¸ è§†é¢‘æ€»ç»“å®Œæˆï¼š\n", summary)

        if output_summary_path:
            with open(output_summary_path, "w", encoding="utf-8") as f:
                json.dump({"summary": summary}, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ€»ç»“å·²ä¿å­˜è‡³ï¼š{output_summary_path}")

        return summary

    except Exception as e:
        print("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥æˆ–æœªè¿”å›æœ‰æ•ˆç»“æœ")
        print("è¿”å›å†…å®¹ï¼š", response)
        return None
