import os
import cv2
import torch
import numpy as np
from tqdm import tqdm
from PIL import Image
from cn_clip.clip import load_from_name
import cn_clip.clip as clip
import librosa
import glob
import math


class KeyframeExtractor:
    def __init__(self, device="cuda"):
        self.device = device
        self.model, self.preprocess = load_from_name("ViT-B-16", device=device)
        self.model.eval()

    def extract_keyframes(self, video_path, output_dir, asr_data=None, audio_path=None):
        """
        ä¸‰æ¨¡å¼å…³é”®å¸§æŠ½å–ï¼š
        1. æ–‡æœ¬å¼•å¯¼ï¼ˆæœ‰è¯­éŸ³æ—¶ï¼‰
        2. è§†è§‰è¡¥å¿ï¼ˆé™é»˜åŒºé—´ï¼‰
        3. çº¯è§†è§‰ï¼ˆæ— è¯­éŸ³æ—¶ï¼‰
        """
        # æ£€æµ‹é™é»˜åŒºé—´
        silent_ranges = self._detect_silent_ranges(audio_path) if audio_path else []

        # ğŸ‘‡ è‡ªåŠ¨è¡¥å‰æ®µé™é»˜åŒºåŸŸï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if asr_data and len(asr_data) > 0 and asr_data[0]["start"] > 0:
            print(f"ğŸ” æ£€æµ‹åˆ°å‰æ®µé™é»˜ï¼š0.0 ~ {asr_data[0]['start']} ç§’ï¼Œå°†è‡ªåŠ¨è¡¥å¸§")
            silent_ranges.insert(0, (0.0, asr_data[0]["start"]))

        if asr_data and len(asr_data) > 0:
            if silent_ranges:
                return self._hybrid_extraction(video_path, output_dir, asr_data, silent_ranges)
            return self._text_guided_extraction(video_path, output_dir, asr_data)

        return self._visual_guided_extraction(video_path, output_dir)

    def _hybrid_extraction(self, video_path, output_dir, asr_data, silent_ranges):
        """æ··åˆæ¨¡å¼æŠ½å–ï¼ˆæ–‡æœ¬å¼•å¯¼ + é™é»˜è¡¥å¸§ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        keyframes = []
        processed_frames = set()

        # æ–‡æœ¬å¼•å¯¼æŠ½å–
        text_kf = self._text_guided_extraction(video_path, output_dir, asr_data)
        keyframes.extend(text_kf)
        processed_frames.update(kf["frame_idx"] for kf in text_kf)

        # è§†è§‰è¡¥å¿æŠ½å–ï¼ˆâ€œæ¯2ç§’æŠ½1å¸§ï¼Œå‘ä¸Šå–æ•´â€ï¼‰
        for start, end in silent_ranges:
            duration = end - start
            num_frames = int(np.ceil(duration / 2))
            if num_frames == 0:
                continue

            interval = duration / num_frames  # æ¯å¸§é—´éš”ï¼ˆç§’ï¼‰
            for i in range(num_frames):
                timestamp = start + i * interval
                frame_idx = int(timestamp * fps)

                # âœ… é˜²æ­¢é‡å¤æŠ½å¸§
                if frame_idx in processed_frames:
                    continue
                processed_frames.add(frame_idx)

                # æŠ½å¸§å¹¶å†™å…¥å›¾åƒ
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret:
                    continue

                save_path = os.path.join(output_dir, f"comp_kf_{frame_idx:05d}.jpg")
                cv2.imwrite(save_path, frame)

                keyframes.append({
                    "mode": "visual_compensate",
                    "frame_idx": frame_idx,
                    "frame_rank": len([k for k in keyframes if k["mode"] == "visual_compensate"]),
                    "timestamp": round(frame_idx / fps, 2),
                    "start": start,
                    "end": end,
                    "text": "",
                    "text_len": 0,
                    "importance": self._calc_frame_importance(frame),
                    "image_path": save_path
                })

        cap.release()
        return sorted(keyframes, key=lambda x: x["timestamp"])

    def _text_guided_extraction(self, video_path, output_dir, asr_data):
        """æ–‡æœ¬å¼•å¯¼çš„å…³é”®å¸§æŠ½å–ï¼ˆå«CLIPç‰¹å¾ä¿å­˜ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        keyframes = []

        # åˆ›å»ºç‰¹å¾å­˜å‚¨ç›®å½•
        feat_dir = os.path.join(output_dir, "clip_features")
        os.makedirs(feat_dir, exist_ok=True)

        for seg_idx, seg in enumerate(tqdm(asr_data, desc="æ–‡æœ¬å¼•å¯¼æŠ½å¸§")):
            start_frame = int(seg["start"] * fps)
            end_frame = int(seg["end"] * fps)
            text = seg["text"]

            # åŠ¨æ€è®¡ç®—æŠ½å¸§æ•°é‡ï¼ˆæ ¹æ®æ–‡æœ¬é•¿åº¦ï¼‰
            num_frames = max(1, min(30, len(text) // 20))  # æ¯æ®µæœ€å¤š30å¸§
            step = max(1, (end_frame - start_frame) // (num_frames + 1))
            frame_indices = [start_frame + step * i for i in range(1, num_frames + 1)]

            # æ–‡æœ¬ç‰¹å¾æå–ï¼ˆæå‰è®¡ç®—ï¼‰
            text_input = torch.cat([clip.tokenize(text)]).to(self.device)
            with torch.no_grad():
                text_feat = self.model.encode_text(text_input)
                text_feat /= text_feat.norm(dim=-1, keepdim=True)

            for rank, frame_idx in enumerate(frame_indices):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret: continue

                # å›¾åƒç‰¹å¾æå–
                image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                image_input = self.preprocess(image).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    image_feat = self.model.encode_image(image_input)
                    image_feat /= image_feat.norm(dim=-1, keepdim=True)
                    sim = (image_feat * text_feat).sum().item()

                # ä¿å­˜å…³é”®å¸§å›¾åƒ
                frame_filename = f"text_kf_{seg_idx:03d}_{frame_idx:05d}.jpg"
                save_path = os.path.join(output_dir, frame_filename)
                cv2.imwrite(save_path, frame)

                # ä¿å­˜CLIPç‰¹å¾ï¼ˆæ–°å¢ï¼‰
                feat_filename = f"feat_{seg_idx:03d}_{frame_idx:05d}.pt"
                torch.save({
                    "image_feat": image_feat.cpu(),
                    "text_feat": text_feat.cpu()
                }, os.path.join(feat_dir, feat_filename))

                keyframes.append({
                    "mode": "text_guided",
                    "segment_idx": seg_idx,
                    "frame_idx": frame_idx,
                    "frame_rank": rank,
                    "timestamp": round(frame_idx / fps, 2),
                    "start": seg["start"],
                    "end": seg["end"],
                    "text": text,
                    "text_len": len(text),
                    "similarity": round(sim, 4),
                    "image_path": save_path,
                    "feat_path": os.path.join(feat_dir, feat_filename)  # æ–°å¢ç‰¹å¾è·¯å¾„
                })

        cap.release()

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾åˆ°å•ä¸ªæ–‡ä»¶ï¼ˆé€‚é…è®­ç»ƒï¼‰
        self._compile_features(feat_dir, os.path.join(output_dir, "clip_features.pth"))

        return keyframes

    def _compile_features(self, feat_dir, output_path):
        """å°†æ‰€æœ‰ç‰¹å¾ç¼–è¯‘ä¸ºè®­ç»ƒç”¨çš„.pthæ–‡ä»¶"""
        image_feats = []
        text_feats = []

        for feat_file in sorted(glob.glob(os.path.join(feat_dir, "*.pt"))):
            data = torch.load(feat_file)
            image_feats.append(data["image_feat"])
            text_feats.append(data["text_feat"])

        torch.save({
            "image_feats": torch.cat(image_feats),
            "text_feats": torch.cat(text_feats)
        }, output_path)

        print(f"âœ… CLIPç‰¹å¾å·²ç¼–è¯‘ä¿å­˜è‡³ {output_path}")

#åŸæ¥çš„text_guided,ä¸å¸¦_compile_features_
    # def _text_guided_extraction(self, video_path, output_dir, asr_data):
    #     """æ–‡æœ¬å¼•å¯¼æ¨¡å¼ï¼ˆä¿ç•™æ‰€æœ‰å­—æ®µï¼‰"""
    #     cap = cv2.VideoCapture(video_path)
    #     fps = cap.get(cv2.CAP_PROP_FPS)
    #     keyframes = []
    #
    #     for seg_idx, seg in enumerate(tqdm(asr_data, desc="æ–‡æœ¬å¼•å¯¼æŠ½å¸§")):
    #         start_frame = int(seg["start"] * fps)
    #         end_frame = int(seg["end"] * fps)
    #         text = seg["text"]
    #
    #         # åŠ¨æ€æŠ½å¸§
    #         num_frames = max(1, len(text) // 20)
    #         step = max(1, (end_frame - start_frame) // (num_frames + 1))
    #         frame_indices = [start_frame + step * i for i in range(1, num_frames + 1)]
    #
    #         # æ–‡æœ¬ç‰¹å¾
    #         text_input = torch.cat([clip.tokenize(text)]).to(self.device)
    #         with torch.no_grad():
    #             text_feat = self.model.encode_text(text_input)
    #             text_feat /= text_feat.norm(dim=-1, keepdim=True)
    #
    #         for rank, frame_idx in enumerate(frame_indices):
    #             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
    #             ret, frame = cap.read()
    #             if not ret: continue
    #
    #             # è®¡ç®—ç›¸ä¼¼åº¦
    #             image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    #             image_input = self.preprocess(image).unsqueeze(0).to(self.device)
    #             with torch.no_grad():
    #                 image_feat = self.model.encode_image(image_input)
    #                 image_feat /= image_feat.norm(dim=-1, keepdim=True)
    #                 sim = (image_feat * text_feat).sum().item()
    #
    #             # ä¿å­˜ç»“æœï¼ˆä¿ç•™æ‰€æœ‰å­—æ®µï¼‰
    #             save_path = os.path.join(output_dir, f"text_kf_{seg_idx:03d}_{frame_idx:05d}.jpg")
    #             cv2.imwrite(save_path, frame)
    #             keyframes.append({
    #                 "mode": "text_guided",
    #                 "segment_idx": seg_idx,
    #                 "frame_idx": frame_idx,
    #                 "frame_rank": rank,
    #                 "timestamp": round(frame_idx / fps, 2),
    #                 "start": seg["start"],
    #                 "end": seg["end"],
    #                 "text": text,
    #                 "text_len": len(text),
    #                 "similarity": round(sim, 4),
    #                 "image_path": save_path
    #             })
    #
    #     cap.release()
    #     return keyframes

    def _visual_guided_extraction(self, video_path, output_dir, num_frames=10):
        """çº¯è§†è§‰æ¨¡å¼ï¼ˆæ–°å¢å­—æ®µï¼‰"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_scores = []

        # è®¡ç®—å¸§é‡è¦æ€§
        for i in tqdm(range(total_frames), desc="è§†è§‰å¼•å¯¼æŠ½å¸§"):
            ret, frame = cap.read()
            if not ret: break
            frame_scores.append((i, self._calc_frame_importance(frame)))

        # å‡è¡¡é€‰å–
        selected_indices = []
        step = total_frames // num_frames
        for i in range(num_frames):
            start = i * step
            end = (i + 1) * step
            candidates = [idx for idx, _ in frame_scores if start <= idx < end]
            if candidates: selected_indices.append(candidates[0])

        # ä¿å­˜ç»“æœ
        keyframes = []
        for rank, idx in enumerate(sorted(selected_indices)):
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            _, frame = cap.read()
            save_path = os.path.join(output_dir, f"visual_kf_{idx:05d}.jpg")
            cv2.imwrite(save_path, frame)
            keyframes.append({
                "mode": "visual_guided",
                "frame_idx": idx,
                "frame_rank": rank,
                "timestamp": round(idx / fps, 2),
                "start": idx / fps,
                "end": (idx + 1) / fps,
                "text": "",
                "text_len": 0,
                "importance": frame_scores[idx][1],
                "image_path": save_path
            })

        cap.release()
        return keyframes

    def _calc_frame_importance(self, frame):
        """è§†è§‰é‡è¦æ€§è®¡ç®—ï¼ˆä¸å˜ï¼‰"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
        hist /= hist.sum() + 1e-10
        entropy = -np.sum(hist * np.log2(hist + 1e-10))

        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            features = self.model.encode_image(image_input)
            clip_score = features.norm().item()

        return 0.7 * entropy + 0.3 * clip_score

    def _detect_silent_ranges(self, audio_path, min_silence_duration=2.0):
        """
        åŸºäº ASR JSON æ–‡ä»¶ + éŸ³é¢‘æ—¶é•¿ï¼Œæ¨æ–­é™é»˜åŒºé—´ï¼ˆå•ä½ï¼šç§’ï¼‰
        - min_silence_durationï¼šåˆ¤æ–­é™é»˜çš„æœ€å°é—´éš”ï¼ˆç§’ï¼‰
        """
        import json

        # ğŸ”§ è·å–éŸ³é¢‘åï¼ˆä¸å¸¦åç¼€ï¼‰
        video_name = os.path.splitext(os.path.basename(audio_path))[0]

        # ğŸ—‚ æ„å»ºé»˜è®¤è¾“å‡ºç›®å½•è·¯å¾„
        output_dir = os.path.join(os.path.dirname(audio_path), f"CNCLIP_keyframes_{video_name}")
        json_path = os.path.join(output_dir, f"{video_name}.json")

        if not os.path.exists(json_path):
            print(f"âŒ æœªæ‰¾åˆ° ASR JSON æ–‡ä»¶: {json_path}")
            return []

        with open(json_path, "r", encoding="utf-8") as f:
            asr_segments = json.load(f)

        # ğŸ“ è·å–éŸ³é¢‘æ—¶é•¿
        y, sr = librosa.load(audio_path, sr=None)
        total_duration = len(y) / sr

        # ğŸ§  è®¡ç®—é™é»˜åŒºé—´
        silence_ranges = []
        last_end = 0.0

        for seg in asr_segments:
            current_start = seg["start"]
            if current_start - last_end >= min_silence_duration:
                silence_ranges.append((last_end, current_start))
            last_end = seg["end"]

        if total_duration - last_end >= min_silence_duration:
            silence_ranges.append((last_end, total_duration))

        print(f"âœ… å…±æ£€æµ‹åˆ°é™é»˜åŒºé—´ {len(silence_ranges)} æ®µï¼š{silence_ranges}")
        return silence_ranges
