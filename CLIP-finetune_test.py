import torch
from cn_clip.clip import load_from_name, tokenize
from PIL import Image

# ==== Step 1: åŠ è½½æ¨¡å‹ç»“æ„å’Œé¢„å¤„ç†å™¨ ====
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ç»“æ„...")
model, preprocess = load_from_name("ViT-B-16", download_root="./")
model.cuda().eval()

# ==== Step 2: åŠ è½½ä½ è‡ªå·±è®­ç»ƒå¥½çš„ checkpoint ====
ckpt_path = r"G:\videochat\my_design\pretrained_weights\epoch_latest.pt"  # â† â† æ›¿æ¢ä¸ºä½ æ¨¡å‹çš„è·¯å¾„
print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æƒé‡ï¼š{ckpt_path}")
checkpoint = torch.load(ckpt_path, map_location="cpu")

# æ³¨æ„ï¼šå¦‚æœæ¨¡å‹æ˜¯ DDP ä¿å­˜çš„ï¼Œéœ€è¦å»æ‰å‰ç¼€ "module."
state_dict = {k.replace("module.", ""): v for k, v in checkpoint["state_dict"].items()}
model.load_state_dict(state_dict)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ==== Step 3: å‡†å¤‡å›¾åƒè¾“å…¥ ====
image_path = r"G:\videochat\my_design\demo.jpg"  # â† â† æ›¿æ¢ä¸ºä½ è¦æµ‹è¯•çš„å›¾åƒ
image = preprocess(Image.open(image_path)).unsqueeze(0).cuda()

# ==== Step 4: å‡†å¤‡æ–‡æœ¬è¾“å…¥ ====
texts = ["ä¸€åªåœ¨æ²™å‘ä¸Šçš„çŒ«", "ä¸€è¾†çº¢è‰²è·‘è½¦", "ä¸€ä¸ªç©¿ç™½è¡£æœçš„äºº"]  # ä½ å¯ä»¥éšä¾¿æ¢
text_tokens = tokenize(texts).cuda()

# ==== Step 5: æ¨¡å‹æ¨ç† ====
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text_tokens)

    # å½’ä¸€åŒ–
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # ç›¸ä¼¼åº¦åˆ†æ•°
    similarity = (100.0 * image_features @ text_features.T)
    probs = similarity.softmax(dim=-1)

# ==== Step 6: æ‰“å°ç»“æœ ====
print("ğŸ“Š å›¾åƒ vs æ–‡æœ¬ ç›¸ä¼¼åº¦ï¼ˆè¶Šé«˜è¶Šç›¸å…³ï¼‰ï¼š")
for i, score in enumerate(probs[0]):
    print(f'  - "{texts[i]}"ï¼š{score.item():.4f}')
