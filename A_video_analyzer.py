import os
import json
import torch
import time
from A_audio_extractor import extract_audio_from_video
from A_audio_recognition import transcribe_audio
from A_keyframe_extractor import extract_keyframes_with_clip
from A_model_inference import summarize_video_from_all_frames,build_structured_prompt,generate_video_summary
from cn_clip.clip import load_from_name

# ===========================
# è§†é¢‘åˆ†æä¸»æµç¨‹ï¼ˆé‡æ„ç‰ˆï¼‰
# ===========================
def process_video(video_path, output_dir, api_key):
    start_time = time.time()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # éŸ³é¢‘è·¯å¾„è®¾ç½®
    audio_path = video_path.replace(".mp4", ".mp3")

    # Step 1: æå–éŸ³é¢‘
    audio_path = extract_audio_from_video(video_path, audio_path)
    print(f"ğŸµ éŸ³é¢‘æå–å®Œæˆï¼š{audio_path}")

    # Step 2: è¯­éŸ³è¯†åˆ«
    transcription = transcribe_audio(audio_path, api_key)
    print(f"ğŸ“ è¯­éŸ³è½¬å½•å®Œæˆï¼Œå…± {len(transcription)} æ¡å¥å­")

    # Step 3: åŠ è½½ CLIP æ¨¡å‹
    print("ğŸ§  æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = load_from_name("ViT-B-16", device=device)
    model.eval()

    # Step 4: æå–å…³é”®å¸§å¹¶åŒ¹é…è¯­éŸ³ç‰‡æ®µ
    keyframes_combined = extract_keyframes_with_clip(
        video_path, output_dir, transcription, model, preprocess, device
    )

    # ä¿å­˜å…³é”®å¸§ä¿¡æ¯
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    keyframe_json_path = os.path.join(output_dir, f"{video_name}_cnclip.json")
    with open(keyframe_json_path, "w", encoding="utf-8") as f:
        json.dump(keyframes_combined, f, ensure_ascii=False, indent=2)
    print(f"âœ… å…³é”®å¸§+æ–‡æœ¬ä¿¡æ¯å·²ä¿å­˜ï¼š{keyframe_json_path}")

    # Step 5: è§†é¢‘æ€»ç»“
    summary_output_path = os.path.join(output_dir, f"{video_name}_summary.json")
    summarize_video_from_all_frames(keyframes_combined, api_key, output_summary_path=summary_output_path)
    # for idx, frame_info in enumerate(keyframes_combined):
    #     is_last = (idx == len(keyframes_combined) - 1)
    #     prompt = build_structured_prompt(frame_info, is_last=is_last)
    #     print(prompt)
    #     image_path = frame_info["image_path"]
    #     summary = generate_video_summary(image_path, prompt, api_key)
    #     print(f"ğŸ¬ è§†é¢‘å†…å®¹æ€»ç»“ï¼š{summary}")

    print(f"ğŸ“„ è§†é¢‘æ€»ç»“å®Œæˆï¼š{summary_output_path}")

    # Step 6: è¯äº‘æˆ–æ€ç»´å¯¼å›¾ç”Ÿæˆï¼ˆæ¥å£ä¿ç•™ï¼‰
    # from A_visualizer import generate_wordcloud_or_mindmap
    # generate_wordcloud_or_mindmap(transcription, output_dir)

    # Step 7: è§†é¢‘äº‹ä»¶å®šä½ä¸è¯†åˆ«ï¼ˆæ¥å£ä¿ç•™ï¼‰
    # from A_event_detector import detect_video_events
    # events = detect_video_events(transcription, keyframes_combined)
    # events_output_path = os.path.join(output_dir, f"{video_name}_events.json")
    # with open(events_output_path, "w", encoding="utf-8") as f:
    #     json.dump(events, f, ensure_ascii=False, indent=2)
    # print(f"ğŸ“Œ äº‹ä»¶æ£€æµ‹ç»“æœä¿å­˜ï¼š{events_output_path}")

    end_time = time.time()
    print(f"â±ï¸ å…¨æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")


# ========== è„šæœ¬å…¥å£ ==========
if __name__ == "__main__":
    video_path = r'G:\videochat\my_design\video_without_audio.mp4'
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    output_dir = f"G:/videochat/my_design/CNCLIP_keyframes_{video_name}"
    api_key = "sk-e6f5a000ba014f92b4857a6dcd782591"

    process_video(video_path, output_dir, api_key)
