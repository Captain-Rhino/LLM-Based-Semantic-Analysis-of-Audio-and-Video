import os
import json
import torch
import time
from A_audio_extractor import extract_audio_from_video
from A_audio_recognition import transcribe_audio
from A_keyframe_extractor import KeyframeExtractor
from A_model_inference import summarize_video_from_all_frames
from A_visualizer import generate_wordcloud,generate_mindmap_from_summary
from cn_clip.clip import load_from_name
from A_clip_finetune import train_adaptor  # æ–°å¢å¯¼å…¥


def process_video(video_path, output_dir, api_key, do_finetune=False):  # æ–°å¢do_finetuneå‚æ•°
    start_time = time.time()
    #æ–°å»ºæ–‡ä»¶è·¯å¾„
    os.makedirs(output_dir, exist_ok=True)

    # Step 1-3: åŸæœ‰éŸ³é¢‘å¤„ç†å’ŒCLIPåŠ è½½ä¸å˜
    #step1:æå–éŸ³é¢‘mp3
    audio_path = extract_audio_from_video(video_path, video_path.replace(".mp4", ".mp3"))

    #step2:è½¬å½•æ–‡æœ¬å¹¶ä¿å­˜
    transcription = transcribe_audio(audio_path, api_key)
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    transcription_save_path = os.path.join(output_dir, f"{video_name}.json")

    # ä¿å­˜ä¸º JSON æ–‡ä»¶
    with open(transcription_save_path, "w", encoding="utf-8") as f:
        json.dump(transcription, f, ensure_ascii=False, indent=2)

    print(f"âœ… è¯­éŸ³è½¬å½•ç»“æœå·²ä¿å­˜è‡³ï¼š{transcription_save_path}")


    #step3:åŠ è½½clip
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = load_from_name("ViT-B-16", device=device)
    model.eval()

    # Step 4: å…³é”®å¸§æå–ï¼ˆè‡ªåŠ¨ä¿å­˜CLIPç‰¹å¾ï¼‰
    extractor = KeyframeExtractor(device=device)
    keyframes = extractor.extract_keyframes(
        video_path=video_path,
        output_dir=output_dir,
        asr_data=transcription,
        audio_path=audio_path,
    )
    #ä¿å­˜jsonæ–‡ä»¶
    keyframe_json_path = os.path.join(
        output_dir,
        f"{os.path.splitext(os.path.basename(video_path))[0]}_cnclip.json"
    )
    with open(keyframe_json_path, "w", encoding="utf-8") as f:
        json.dump(keyframes, f, ensure_ascii=False, indent=2)
    print(f"âœ… å…³é”®å¸§ä¿¡æ¯å·²ä¿å­˜è‡³ï¼š{keyframe_json_path}")

    # æ–°å¢è®­ç»ƒç¯èŠ‚
    if do_finetune:  # åªæœ‰å¼€å¯æ ‡å¿—æ—¶æ‰è®­ç»ƒ
        print("\nğŸ”§ å¼€å§‹CLIPé€‚é…å±‚å¾®è°ƒ...")
        train_data_path = os.path.join(output_dir, "clip_features.pth")
        adaptor = train_adaptor(
            data_path=train_data_path,
            epochs=10,
            batch_size=8
        )
        # ä¿å­˜é€‚é…å±‚æƒé‡ä¾›åç»­ä½¿ç”¨
        torch.save(adaptor.state_dict(), os.path.join(output_dir, "clip_adaptor.pth"))

    # Step 5: è§†é¢‘æ€»ç»“ï¼ˆè‡ªåŠ¨æ£€æµ‹æ˜¯å¦å­˜åœ¨é€‚é…å±‚ï¼‰
    #
    adaptor_path = (
        os.path.join(output_dir, "clip_adaptor.pth")
        if do_finetune else
        os.path.join("G:/videochat/my_design/adaptor_results", "best_adaptor.pth")  # â† æ”¹è¿™é‡Œ
    )

    if not os.path.exists(adaptor_path):
        adaptor_path = None  # å¦‚æœæ‰¾ä¸åˆ°è·¯å¾„å°±ä¸ä¼ 

    summary_output_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(video_path))[0]}_summary.json")
    summarize_video_from_all_frames(
        keyframes,
        api_key,
        adaptor_path=adaptor_path,  # ä¼ é€’é€‚é…å±‚è·¯å¾„
        output_summary_path=summary_output_path
    )
    # summarize_video_from_all_frames(
    #     keyframes,
    #     api_key,
    #     adaptor_path=os.path.join(output_dir, "clip_adaptor.pth") if do_finetune else None,  # ä¼ é€’é€‚é…å±‚è·¯å¾„
    #     output_summary_path=summary_output_path
    # )

    # Step 6: å¯è§†åŒ–ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰

    generate_wordcloud(transcription, summary_output_path, output_dir,
                       os.path.splitext(os.path.basename(video_path))[0])
    generate_mindmap_from_summary(summary_output_path, output_dir, video_name)

    print(f"â±ï¸ å…¨æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{time.time() - start_time:.2f} ç§’")


if __name__ == "__main__":
    video_path = r'G:\videochat\my_design\test_2.mp4'
    output_dir = f"G:/videochat/my_design/CNCLIP_keyframes_{os.path.splitext(os.path.basename(video_path))[0]}"
    api_key = "sk-e6f5a000ba014f92b4857a6dcd782591"

    # æ–°å¢--finetuneå‚æ•°æ§åˆ¶æ˜¯å¦è®­ç»ƒ
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--finetune", action="store_true", help="Enable CLIP adaptor fine-tuning")
    args = parser.parse_args()

    process_video(video_path, output_dir, api_key, do_finetune=args.finetune)