# A_event_detector.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import os
import cv2
from PIL import Image

# å¯¼å…¥éŸ³é¢‘ç‰¹å¾æå–å™¨
from feature_extractor import extract_audio_features, get_audio_segment

# --- 1. äº‹ä»¶ç±»åˆ«å®šä¹‰ (é‡è¦ï¼šè®­ç»ƒå’Œæ¨ç†å¿…é¡»ä¸€è‡´) ---
EVENT_CLASSES = ["speech", "applause", "music", "silence", "laughter", "background"]
NUM_CLASSES = len(EVENT_CLASSES)
EVENT_TO_ID = {event: i for i, event in enumerate(EVENT_CLASSES)}
ID_TO_EVENT = {i: event for i, event in enumerate(EVENT_CLASSES)}

VISUAL_DIM = 512  # CN-CLIP ViT-B/16 è¾“å‡ºç»´åº¦
AUDIO_DIM = 128   # VGGish è¾“å‡ºç»´åº¦
FUSED_DIM = VISUAL_DIM + AUDIO_DIM # èåˆåçš„ç»´åº¦

# --- 2. åˆ†ç±»å™¨æ¨¡å‹å®šä¹‰ (è®­ç»ƒå’Œæ¨ç†éƒ½éœ€è¦è¿™ä¸ªç»“æ„) ---
class EventClassifier(nn.Module):
    def __init__(self, input_dim=FUSED_DIM, num_classes=NUM_CLASSES):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# --- 3. äº‹ä»¶æ£€æµ‹æ¨ç†å‡½æ•° ---
def detect_events_in_video(video_path, audio_path, trained_model_path,
                           clip_model, clip_preprocess, device,
                           segment_duration=1.0, batch_size=16):
    """
    å¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œäº‹ä»¶æ£€æµ‹ï¼ˆæ¨ç†ï¼‰ã€‚
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæå–ç‰¹å¾ï¼Œè¿›è¡Œé¢„æµ‹ã€‚
    """
    print(f"ğŸ” å¼€å§‹å¯¹è§†é¢‘ {os.path.basename(video_path)} è¿›è¡Œäº‹ä»¶æ£€æµ‹...")
    # --- a) åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨ ---
    print(f"   åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {trained_model_path}")
    if not os.path.exists(trained_model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {trained_model_path}")
        return []

    model = EventClassifier(input_dim=FUSED_DIM, num_classes=NUM_CLASSES)
    try:
        model.load_state_dict(torch.load(trained_model_path, map_location=device))
        model.to(device)
        model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("   æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
        return []

    # --- b) è·å–è§†é¢‘ä¿¡æ¯ ---
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ é”™è¯¯: æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ {video_path}")
            return []
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps
        cap.release()
        print(f"   è§†é¢‘æ—¶é•¿: {duration:.2f} ç§’, FPS: {fps:.2f}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: è¯»å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        return []

    # --- c) å‡†å¤‡æ—¶é—´æˆ³å’Œç‰¹å¾æå– ---
    results = []
    timestamps = np.arange(0, duration, segment_duration)
    all_features = []

    print(f"   æŒ‰ {segment_duration}s çª—å£æå–éŸ³è§†é¢‘ç‰¹å¾...")
    # ç¡®ä¿ CLIP æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
    clip_model.eval()

    # --- ä¼˜åŒ–ï¼šç¼“å­˜ VideoCapture å¯¹è±¡ ---
    cap = None
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError("æ— æ³•æ‰“å¼€è§†é¢‘")

        for i in tqdm(range(len(timestamps)), desc="   ç‰¹å¾æå–è¿›åº¦"):
            start_time = timestamps[i]
            end_time = start_time + segment_duration

            # i) è§†è§‰ç‰¹å¾
            visual_feature = np.zeros(VISUAL_DIM) # é»˜è®¤é›¶å‘é‡
            try:
                mid_frame_idx = int((start_time + segment_duration / 2) * fps)
                # æ£€æŸ¥å¸§ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                if 0 <= mid_frame_idx < frame_count:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_idx)
                    success, frame = cap.read()
                    if success:
                        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                        img_tensor = clip_preprocess(img_pil).unsqueeze(0).to(device)
                        with torch.no_grad():
                            visual_feature = clip_model.encode_image(img_tensor).squeeze().cpu().numpy()
                    # else: print(f"   è­¦å‘Š: å¸§ {mid_frame_idx} è¯»å–å¤±è´¥")
                # else: print(f"   è­¦å‘Š: è®¡ç®—çš„å¸§ç´¢å¼• {mid_frame_idx} è¶…å‡ºèŒƒå›´ [0, {frame_count-1}]")

            except Exception as e:
                print(f"   è­¦å‘Š: æå– {start_time:.2f}s å¤„è§†è§‰ç‰¹å¾æ—¶å‡ºé”™: {e}")
                # ä¿ç•™é›¶å‘é‡

            # ii) éŸ³é¢‘ç‰¹å¾
            audio_feature = np.zeros(AUDIO_DIM) # é»˜è®¤é›¶å‘é‡
            try:
                audio_segment, sr = get_audio_segment(audio_path, start_time, end_time)
                if audio_segment is not None and len(audio_segment) > 0:
                    # ä½¿ç”¨ A_feature_extractor ä¸­çš„å‡½æ•°
                    audio_feature_extracted = extract_audio_features(audio_segment, sr)
                    if audio_feature_extracted is not None:
                        audio_feature = audio_feature_extracted
                    # else: print(f"   è­¦å‘Š: {start_time:.2f}s å¤„éŸ³é¢‘ç‰¹å¾æå–è¿”å› None")
                # else: print(f"   è­¦å‘Š: {start_time:.2f}s å¤„æ— æ³•åŠ è½½éŸ³é¢‘ç‰‡æ®µ")

            except Exception as e:
                print(f"   è­¦å‘Š: æå– {start_time:.2f}s å¤„éŸ³é¢‘ç‰¹å¾æ—¶å‡ºé”™: {e}")
                # ä¿ç•™é›¶å‘é‡

            # iii) èåˆç‰¹å¾
            fused_feature = np.concatenate((visual_feature, audio_feature))
            all_features.append(fused_feature)

    except Exception as e:
        print(f"âŒ é”™è¯¯: ç‰¹å¾æå–ä¸»å¾ªç¯å‡ºé”™: {e}")
        # ç¡®ä¿ VideoCapture è¢«é‡Šæ”¾
        if cap is not None and cap.isOpened():
            cap.release()
        return []
    finally:
        # ç¡®ä¿ VideoCapture è¢«é‡Šæ”¾
        if cap is not None and cap.isOpened():
            cap.release()

    # --- d) æ‰¹é‡æ¨ç† ---
    if not all_features:
        print("   æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç‰¹å¾ï¼Œæ— æ³•è¿›è¡Œäº‹ä»¶æ£€æµ‹ã€‚")
        return []

    feature_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(all_features), dtype=torch.float32))
    feature_loader = DataLoader(feature_dataset, batch_size=batch_size)

    all_predictions = []
    all_confidences = []
    print(f"   ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ‰¹é‡æ¨ç† (Batch Size: {batch_size})...")
    with torch.no_grad():
        for batch_features in tqdm(feature_loader, desc="   æ¨ç†è¿›åº¦"):
            batch_features = batch_features[0].to(device) # DataLoader è¿”å›çš„æ˜¯å…ƒç»„
            outputs = model(batch_features)
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predicted_ids = torch.max(probabilities, dim=1)
            all_predictions.extend(predicted_ids.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())

    # --- e) æ•´ç†åŸå§‹ç»“æœ ---
    raw_events = []
    for i in range(len(timestamps)):
        start_time = timestamps[i]
        # ç¡®ä¿ç»“æŸæ—¶é—´ä¸è¶…è¿‡è§†é¢‘æ€»æ—¶é•¿
        end_time = min(start_time + segment_duration, duration)
        pred_id = all_predictions[i]
        confidence = round(float(all_confidences[i]), 4)
        event_label = ID_TO_EVENT.get(pred_id, "unknown") # ä½¿ç”¨ get ä»¥é˜²ä¸‡ä¸€
        raw_events.append({
            "start": round(start_time, 2),
            "end": round(end_time, 2),
            "event": event_label,
            "confidence": confidence
        })
    print("   åŸå§‹äº‹ä»¶é¢„æµ‹å®Œæˆã€‚")
    return raw_events


# --- 4. äº‹ä»¶åå¤„ç† (åˆå¹¶) ---
def merge_events(raw_events, min_confidence=0.5, min_duration=0.8):
    """
    åˆå¹¶ç›¸é‚»çš„ã€ç½®ä¿¡åº¦é«˜çš„ç›¸åŒäº‹ä»¶ã€‚
    min_duration: åˆå¹¶åäº‹ä»¶çš„æœ€çŸ­æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    """
    if not raw_events:
        return []
    print(f"   å¼€å§‹åˆå¹¶äº‹ä»¶ (min_confidence={min_confidence}, min_duration={min_duration}s)...")

    merged = []
    if not raw_events: return merged

    current_event_type = None
    current_event_start = -1
    current_event_end = -1
    confidences_list = []

    for i, event in enumerate(raw_events):
        is_significant = (event["event"] != "background" and event["confidence"] >= min_confidence)

        if is_significant:
            if event["event"] == current_event_type:
                # æ‰©å±•å½“å‰äº‹ä»¶æ®µ
                current_event_end = event["end"]
                confidences_list.append(event["confidence"])
            else:
                # ç»“æŸä¸Šä¸€ä¸ªäº‹ä»¶æ®µï¼ˆå¦‚æœå­˜åœ¨ä¸”æ»¡è¶³æœ€çŸ­æ—¶é•¿ï¼‰
                if current_event_type is not None and (current_event_end - current_event_start >= min_duration):
                    avg_confidence = round(np.mean(confidences_list), 4)
                    merged.append({
                        "type": current_event_type,
                        "start": round(current_event_start, 2),
                        "end": round(current_event_end, 2),
                        "avg_confidence": avg_confidence,
                        "duration": round(current_event_end - current_event_start, 2)
                    })

                # å¼€å§‹æ–°çš„äº‹ä»¶æ®µ
                current_event_type = event["event"]
                current_event_start = event["start"]
                current_event_end = event["end"]
                confidences_list = [event["confidence"]]
        else:
            # éæ˜¾è‘—äº‹ä»¶ï¼ˆèƒŒæ™¯æˆ–ä½ç½®ä¿¡åº¦ï¼‰å¼ºåˆ¶ç»“æŸä¸Šä¸€ä¸ªäº‹ä»¶æ®µ
            if current_event_type is not None and (current_event_end - current_event_start >= min_duration):
                avg_confidence = round(np.mean(confidences_list), 4)
                merged.append({
                    "type": current_event_type,
                    "start": round(current_event_start, 2),
                    "end": round(current_event_end, 2),
                    "avg_confidence": avg_confidence,
                    "duration": round(current_event_end - current_event_start, 2)
                })
            # é‡ç½®å½“å‰äº‹ä»¶çŠ¶æ€
            current_event_type = None
            current_event_start = -1
            current_event_end = -1
            confidences_list = []

    # å¤„ç†å¾ªç¯ç»“æŸåå¯èƒ½é—ç•™çš„æœ€åä¸€ä¸ªäº‹ä»¶æ®µ
    if current_event_type is not None and (current_event_end - current_event_start >= min_duration):
        avg_confidence = round(np.mean(confidences_list), 4)
        merged.append({
            "type": current_event_type,
            "start": round(current_event_start, 2),
            "end": round(current_event_end, 2),
            "avg_confidence": avg_confidence,
            "duration": round(current_event_end - current_event_start, 2)
        })

    print(f"   äº‹ä»¶åˆå¹¶å®Œæˆï¼Œå¾—åˆ° {len(merged)} ä¸ªäº‹ä»¶ã€‚")
    return merged