# âœ… éœ€è¦æå‰å®‰è£…çš„ä¾èµ–ï¼š
# pip install transformers accelerate torchvision torch

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import json
import os

# ====== é…ç½®æ¨¡å‹ ======
model_name = "llava-hf/llava-llama-3-8b"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ å½“å‰ä½¿ç”¨è®¾å¤‡ï¼š{device}")

# ====== åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ ======
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = LlavaForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    low_cpu_mem_usage=True
).to(device)
processor = AutoProcessor.from_pretrained(model_name)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ====== åŠ è½½å…³é”®å¸§ + æ–‡æœ¬æ•°æ® ======
input_json = "CNCLIP_keyframes_test_movie/test_movie_cnclip.json"  # ä½ ä¹‹å‰ç”Ÿæˆçš„ JSON
with open(input_json, "r", encoding="utf-8") as f:
    data = json.load(f)

results = []

# ====== æ‰§è¡Œé€å¸§æ€»ç»“ ======
for i, item in enumerate(data):
    image_path = item['image_path']
    text = item['text']
    print(f"\nğŸ–¼ï¸ æ­£åœ¨åˆ†æç¬¬ {i} æ®µæ–‡å­— + å›¾ç‰‡ï¼š{image_path}")

    image = Image.open(image_path).convert("RGB")

    prompt = f"è¿™æ˜¯ä¸€å¸§è§†é¢‘æˆªå›¾ï¼Œå­—å¹•ä¸ºï¼š'{text}'ã€‚è¯·ç»“åˆå›¾ç‰‡å†…å®¹æ€»ç»“è¿™æ®µè§†é¢‘å†…å®¹ã€‚"

    inputs = processor(prompt, image, return_tensors="pt").to(device)
    output = model.generate(**inputs, max_new_tokens=100)
    response = processor.batch_decode(output, skip_special_tokens=True)[0]

    print("ğŸ§  æ€»ç»“ï¼š", response)
    results.append({
        "segment_index": item["segment_index"],
        "image_path": image_path,
        "original_text": text,
        "summary": response
    })

# ====== ä¿å­˜ç»“æœ ======
output_path = "video_summary_output.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"\nâœ… æ‰€æœ‰æ‘˜è¦å·²ä¿å­˜åˆ°ï¼š{output_path}")
